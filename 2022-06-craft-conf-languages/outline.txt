Opening slide
-------------

Introduce myself. Doug Creager, Semantic Code team at GitHub. Part of the "Code
Productivity" group. Code Nav, Code Search, Vulnerability Analysis, etc.

Want these features to be available for _everyone_. That means that we're
exposed to all of the programming languages that are hosted on GitHub. Which, if
you think about it, is basically the same as "all of the programming languages
in the world". So we have an interesting, very wide, perspective on languages,
which I hope to share with you today.

Tower of Babel example
----------------------

We often like to compare programming languages with spoken languages, so I want
to start with a famous myth from the Old Testament. This is a painting from the
late 1300s, depicting the Tower of Babel. If you're not familiar with this myth,
the Tower of Babel is meant to explain why people speak different languages. The
story goes that after the Great Flood of Noah, the human world was united as a
single people who spoke the same language. They were so proud of themselves and
their accomplishments that they decided to build a tower tall enough to reach
the heavens. And is often the case in the Old Testament, this is a story of God
punishing humanity for its hubris. In this case, he sundered the people,
spreading them to the corners of the Earth, and causing them to start speaking
many mutually unintelligible languages.

Now, how does this relate to the history of computing and of programming
languages? It is, admittedly, not a perfect analogy. I can't claim that there
was any _single_ programming language that the world's programmers all used in
the "prehistory" of computing. In fact, starting in the 1940s and 50s, there
were several groups simultaneously trying to build human-readable programming
languages on top of machine code and assembly.

[next slide]

But we _can_ say that there are a small handful of languages, like Fortran and
Cobol, that we consider to _share_ the crown of first.

How else is this analogy strained? Well, programmers are certainly guilty of
hubris — but I'm not going to claim that divine retribution sundered the world
of programmers in the 1950s and 1960s. Nevertheless, the end result is the same:

[next slide]

Today, there is a true multitude of programming languages. This wall of logos
might seem like a lot, but it's actually only a small sample of the languages
that exist today. GitHub maintains an open-source package called linguist, which
is a crowd-sourced list of all of the languages that we're aware of — and it
currently contains more than 500 entries!

As an aside, just throwing these logos up here has probably nerd-sniped several
of you, so

[next slide]

let's go ahead and play a game of Pyramid! There are 35 programming language
logos on the screen. Dick Clark wants to know how many of them you recognize!
Just a competition with yourself, it's okay if there many that you aren't
familiar with.

Now, for me, the most interesting part of our Tower of Babel analogy is the
"mutual unintelligibility" part. Like spoken languages, there are _families_ of
programming languages, where languages within a family are similar enough that
familiarity with one means that you're likely to understand at least the basics
of another. Whereas in another family, the very concepts of programming are so
different that it can be almost impossible to understand how those languages
could possibly work!

[next slide]

I would claim that most of us these days are really only familiar with one
programming language and its ecosystem. We're _aware_ that other programming
languages exist, but we don't really take the time to explore other languages to
learn where the similarities and differences are. (And to be clear, that's
perfectly okay! We all have a limited number of hours in the day. Time is the
most finite resource. Learning other languages might genuinely not be the best
use of your time.)

[next slide]

But there is a multitude, and it's one worth exploring. So for the next 20
minutes or so, we're going to explore the multitude together. My hope is that
regardless of which language you're most fluent in, you'll see something new
that makes you think about programming differently. I firmly believe that
knowing about other programming styles and paradigms can make you a more
effective developer.

And also, to clarify, I'm going to be relentlessly positive! This is absolutely
not a sermon about the One True Programming Style that everyone should switch
over to. I'm not going to make any value judgments regarding static or dynamic
typing, for instance. For those of you following along in the chat room or at
home afterwards, please stay positive too!


Recursion vs iteration
----------------------

hero image: https://flic.kr/p/5RQPx9

To get our feet wet, we're going to start by looking at some very high-level
patterns that show up time and again. Our first pattern is repetition: What
facilities do our programming languages give us for performing the same work
over and over?

We'll use a pretty simple example to start with: computing the Fibonacci
numbers. If you're not familiar with this sequence, the first Fibonacci numbers
are 0 and 1. Each subsequent number is the sum of the previous two.

[next slide]

Here we can see an implementation of Fibonacci in Go, using a for loop. We need
to keep track of the previous two values, which we'll call "a" and "b". Then we
loop through, calculating the next value and "sliding" it into our window of
previous values. Once our loop finishes, we know that "a" will contain the
Fibonacci number that was requested.

Go is an example of an "imperative" programming language, where the bulk of your
program consists of a sequence of statements that are executed in order. The for
loop, and its cousin the while loop, are ubiquitous in imperative languages.
Most popular languages these days are imperative. And in fact, we can look at
Fibonacci in [next] Perl, [next] and Python, [next] and C, [next] and C++,
[next] and Rust. And in all of these cases, the implementation looks more or
less identical, just with slight differences in syntax.

[next slide]

But loops aren't the only way to repeat yourself! Another pattern is
"recursion", where you define a function that calls itself. Here we can see a
recursive definition of Fibonacci in Haskell, a "functional" programming
language. A defining characteristic of functional languages is that modifying
variables is prohibited — or at least, greatly frowned upon. One nice feature of
this functional implementation is that it's very similar to the mathematical
definition that we're trying to implement!

This style is not limited to functional languages — we can implement this same
recursive definition in imperative languages too. [next] Here's what it looks
like in C.

One problem with this implementation is that it's slow! It might not be obvious,
but we actually do an exponential amount of work, because we end up calculating
each previous number multiple times. Note that this isn't a problem with
recursion in general, it's just a problem with this particular implementation.

[next slide]

We can fix it, while still being recursive, by maintaining the previous two
values in "a" and "b" variables like we did in the loop implementation. In this
Haskell version, we can't _update_ a and b in the loop body, but we can achieve
the same result by making a recursive call with new values for _parameters_
named a and b. [next] And like before, this pattern is not limited to functional
languages: here it is in C.

Another interesting benefit of the recursive style, and of functional
programming in general, is that it makes it easier to look for higher level
patterns. Not just "oh there are lots of problems that we can solve with
recursion". But going deeper: "How many times do we recurse?" "How much work are
we allowed to do before the recursive call?" "How much work after?"

In the functional programming world, the answers to these questions lead to a
number of different "recursion schemes". Their names can sound strange if you're
not used to them. (Now do you see what I mean about "mutual unintelligibility"
being the most striking part of the Tower of Babel myth?) But those names do
have very specific and precise meanings, which is useful when you're using
functional programming as a lens to explore deep topics in computer science.

[next slide]

Today, I'll just point out that the Fibonacci sequence is an example of a
"histomorphism". The "histo" part has the same root as "history", and signifies
that each recursive step has access to the full history of previously computed
values.

[next slide]

Here's what Fibonacci looks like in Haskell when implemented as a histomorphism.
Like all recursion schemes, you don't actually see any recursion! That's handled
for you by the recursion scheme itself. We just have to specify what should
happen during each recursive step. For a histomorphism, we're given a list of
the previously computed values, and can inspect those values however we need to
compute the next value.


Error handling
--------------

hero image: https://flic.kr/p/B9WA8y

The next pattern we're going to look at is how we handle error conditions in our
programs. You might be surprised to learn how many different error handling
strategies there are!

[next slide]

As our running example in this section, we're going to look at some code that
can parse a single digit, stored in as a single ASCII character, into the
corresponding integer value. This is a process that can fail, since not every
ASCII character is a digit.

[next slide]

Many languages use "exceptions" to handle these errors conditions. Java is one
of the most well-known examples. Here is what our "parse digit" function could
look like in Java. [next] And here it is in Python. [next] and in C++.

You can think of exceptions as aborting the flow of your program. A function
"throws" an exception _instead of_ returning a value. At that point, your
program starts "unwinding" its call stack, looking for some code that knows how
to handle that exception.

[next slide]

We can expand our C++ example to show a parse exception being handled via a
try/catch block. This kind of statement runs the code in the try clause, and if
any part of that code (include anything that it calls) throws an exception, the
catch clauses have a chance to handle the exception. Notice how the exception
might not be handled by the immediate caller of the function that threw the
exception — it might propagate up any number of calls on the call stack before
we find an appropriate handler.

exceptions
enums / either / monad / try


Resources and cleanup
---------------------

hero image: https://flic.kr/p/eRjo3W

GC for memory
malloc + free manually in C
smart pointer / ownership in Rust (GC is not the only automatic memory
management!)

Other resources?

Closing a file after you're done with it
GC finalizers don't work
Need defer or with
RAII in C++ and Rust

Make sure it works in all of your return paths!


Concurrency
-----------

hero image: https://flic.kr/p/NpqGS

OS threads
green threads / goroutines
evented: promises, futures, async
Just do it single threaded! (Coordination always has an overhead)


Conclusion
----------

hero image: https://flic.kr/p/NHDXRb

We've come to the end of our journey. We still have a multitude of programming
languages (and always will!). But hopefully I've been able to add a bit of order
to that multitude for you today.

If you've encountered a new style of programming that used to be completely
alien to you, consider if there are any parts of it that you can bring back to
your native language and ecosystem. If there aren't, you can at least be
thankful that you now have a better, more mindful appreciation for the ways
things are done in your language. And if you do find yourself with some spare
time for personal learning and experimentation, consider implementing a toy
project in a language that's completely outside your comfort zone. It will be
worth the effort!

Thanks for your time!
