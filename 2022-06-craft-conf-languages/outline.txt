Opening slide
-------------

Introduce myself. Doug Creager, Semantic Code team at GitHub. Part of the "Code
Productivity" group. Code Nav, Code Search, Vulnerability Analysis, etc.

Want these features to be available for _everyone_. That means that we're
exposed to all of the programming languages that are hosted on GitHub. Which, if
you think about it, is basically the same as "all of the programming languages
in the world". So we have an interesting, very wide, perspective on languages,
which I hope to share with you today.

Tower of Babel example
----------------------

We often like to compare programming languages with spoken languages, so I want
to start with a famous myth from the Old Testament. This is a painting from the
late 1300s, depicting the Tower of Babel. If you're not familiar with this myth,
the Tower of Babel is meant to explain why people speak different languages. The
story goes that after the Great Flood of Noah, the human world was united as a
single people who spoke the same language. They were so proud of themselves and
their accomplishments that they decided to build a tower tall enough to reach
the heavens. And is often the case in the Old Testament, this is a story of God
punishing humanity for its hubris. In this case, he sundered the people,
spreading them to the corners of the Earth, and causing them to start speaking
many mutually unintelligible languages.

Now, how does this relate to the history of computing and of programming
languages? It is, admittedly, not a perfect analogy. I can't claim that there
was any _single_ programming language that the world's programmers all used in
the "prehistory" of computing. In fact, starting in the 1940s and 50s, there
were several groups simultaneously trying to build human-readable programming
languages on top of machine code and assembly.

[next slide]

But we _can_ say that there are a small handful of languages, like Fortran and
Cobol, that we consider to _share_ the crown of first.

How else is this analogy strained? Well, programmers are certainly guilty of
hubris — but I'm not going to claim that divine retribution sundered the world
of programmers in the 1950s and 1960s. Nevertheless, the end result is the same:

[next slide]

Today, there is a true multitude of programming languages. This wall of logos
might seem like a lot, but it's actually only a small sample of the languages
that exist today. GitHub maintains an open-source package called linguist, which
is a crowd-sourced list of all of the languages that we're aware of — and it
currently contains more than 500 entries!

As an aside, just throwing these logos up here has probably nerd-sniped several
of you, so

[next slide]

let's go ahead and play a game of Pyramid! There are 35 programming language
logos on the screen. Dick Clark wants to know how many of them you recognize!
Just a competition with yourself, it's okay if there many that you aren't
familiar with.

Now, for me, the most interesting part of our Tower of Babel analogy is the
"mutual unintelligibility" part. Like spoken languages, there are _families_ of
programming languages, where languages within a family are similar enough that
familiarity with one means that you're likely to understand at least the basics
of another. Whereas in another family, the very concepts of programming are so
different that it can be almost impossible to understand how those languages
could possibly work!

[next slide]

I would claim that most of us these days are really only familiar with one
programming language and its ecosystem. We're _aware_ that other programming
languages exist, but we don't really take the time to explore other languages to
learn where the similarities and differences are. (And to be clear, that's
perfectly okay! We all have a limited number of hours in the day. Time is the
most finite resource. Learning other languages might genuinely not be the best
use of your time.)

[next slide]

But there is a multitude, and it's one worth exploring. So for the next 20
minutes or so, we're going to explore the multitude together. My hope is that
regardless of which language you're most fluent in, you'll see something new
that makes you think about programming differently. I firmly believe that
knowing about other programming styles and paradigms can make you a more
effective developer.

And also, to clarify, I'm going to be relentlessly positive! This is absolutely
not a sermon about the One True Programming Style that everyone should switch
over to. I'm not going to make any value judgments regarding static or dynamic
typing, for instance. For those of you following along in the chat room or at
home afterwards, please stay positive too!


Recursion vs iteration
----------------------

hero image: https://flic.kr/p/5RQPx9

To get our feet wet, we're going to start by looking at some very high-level
patterns that show up time and again. Our first pattern is repetition: What
facilities do our programming languages give us for performing the same work
over and over?

We'll use a pretty simple example to start with: computing the Fibonacci
numbers. If you're not familiar with this sequence, the first Fibonacci numbers
are 0 and 1. Each subsequent number is the sum of the previous two.

[next slide]

Here we can see an implementation of Fibonacci in Go, using a for loop. We need
to keep track of the previous two values, which we'll call "a" and "b". Then we
loop through, calculating the next value and "sliding" it into our window of
previous values. Once our loop finishes, we know that "a" will contain the
Fibonacci number that was requested.

Go is an example of an "imperative" programming language, where the bulk of your
program consists of a sequence of statements that are executed in order. The for
loop, and its cousin the while loop, are ubiquitous in imperative languages.
Most popular languages these days are imperative. And in fact, we can look at
Fibonacci in [next] Perl, [next] and Python, [next] and C, [next] and C++,
[next] and Rust. And in all of these cases, the implementation looks more or
less identical, just with slight differences in syntax.

[next slide]

But loops aren't the only way to repeat yourself! Another pattern is
"recursion", where you define a function that calls itself. Here we can see a
recursive definition of Fibonacci in Haskell, a "functional" programming
language. A defining characteristic of functional languages is that modifying
variables is prohibited — or at least, greatly frowned upon. One nice feature of
this functional implementation is that it's very similar to the mathematical
definition that we're trying to implement!

This style is not limited to functional languages — we can implement this same
recursive definition in imperative languages too. [next] Here's what it looks
like in C.

One problem with this implementation is that it's slow! It might not be obvious,
but we actually do an exponential amount of work, because we end up calculating
each previous number multiple times. Note that this isn't a problem with
recursion in general, it's just a problem with this particular implementation.

[next slide]

We can fix it, while still being recursive, by maintaining the previous two
values in "a" and "b" variables like we did in the loop implementation. In this
Haskell version, we can't _update_ a and b in the loop body, but we can achieve
the same result by making a recursive call with new values for _parameters_
named a and b. [next] And like before, this pattern is not limited to functional
languages: here it is in C.

Another interesting benefit of the recursive style, and of functional
programming in general, is that it makes it easier to look for higher level
patterns. Not just "oh there are lots of problems that we can solve with
recursion". But going deeper: "How many times do we recurse?" "How much work are
we allowed to do before the recursive call?" "How much work after?"

In the functional programming world, the answers to these questions lead to a
number of different "recursion schemes". Their names can sound strange if you're
not used to them. (Now do you see what I mean about "mutual unintelligibility"
being the most striking part of the Tower of Babel myth?) But those names do
have very specific and precise meanings, which is useful when you're using
functional programming as a lens to explore deep topics in computer science.

[next slide]

Today, I'll just point out that the Fibonacci sequence is an example of a
"histomorphism". The "histo" part has the same root as "history", and signifies
that each recursive step has access to the full history of previously computed
values.

[next slide]

Here's what Fibonacci looks like in Haskell when implemented as a histomorphism.
Like all recursion schemes, you don't actually see any recursion! That's handled
for you by the recursion scheme itself. We just have to specify what should
happen during each recursive step. For a histomorphism, we're given a list of
the previously computed values, and can inspect those values however we need to
compute the next value.


Error handling
--------------

hero image: https://flic.kr/p/B9WA8y

The next pattern we're going to look at is how we handle error conditions in our
programs. You might be surprised to learn how many different error handling
strategies there are!

[next slide]

As our running example in this section, we're going to look at some code that
can parse a single digit, stored in as a single ASCII character, into the
corresponding integer value. This is a process that can fail, since not every
ASCII character is a digit.

[next slide]

Many languages use "exceptions" to handle these errors conditions. Java is one
of the most well-known examples. Here is what our "parse digit" function could
look like in Java. [next] And here it is in Python. [next] and in C++.

You can think of exceptions as aborting the flow of your program. A function
"throws" an exception _instead of_ returning a value. At that point, your
program starts "unwinding" its call stack, looking for some code that knows how
to handle that exception.

[next slide]

We can expand our C++ example to show a parse exception being handled via a
try/catch block. This kind of statement runs the code in the try clause, and if
any part of that code (include anything that it calls) throws an exception, the
catch clauses have a chance to handle the exception. Notice how the exception
might not be handled by the immediate caller of the function that threw the
exception — it might propagate up any number of calls on the call stack before
we find an appropriate handler.

That's one of the defining characteristics of exceptions: your callers get to
decide where and how to handle them, and they might silently propagate through
functions that don't mention exceptions at all. Some people like that, because
it means that many of your functions can focus on the "happy path", and not have
to worry about explicit error handling. Other people don't like that, because
they feel that the implicit control flow is harder to reason about.

[next slide]

Next let's look at the opposite end of the spectrum. Here is how our parse digit
function might look in C, where error handling is very explicit. In C, there is
no _single_ error handling strategy that is used everywhere consistently. A
common pattern is to use special "sentinel" values to indicate errors. Since our
parse digit function only needs the values from 0 to 9, we can use any other
integer result to indicate an error. -1 is often used.

Otherwise, the structure is not really all that different from the version that
uses exceptions. We still need to detect the error condition; we just use a
"return" statement instead of a "throw" statement to signal the error.

[next slide]

The changes are more obvious where we handle the errors. In particular, note
that our parse_file function has to _explicitly_ pass the error condition on to
its caller.

[next slide]

In Go, the overall structure is very similar, but we can take advantage of the
fact that Go functions can return _multiple_ values. That means that we don't
have to reserve a sentinel value to indicate errors, and can encode errors using
their own specific type. Our ParseDigit function returns _both_ a normal value
and an error, and the caller knows that only one or the other should actually be
used.

[next slide]

In Rust, we also use regular values to encode errors, but we can also express
the idea that our parse_digit function returns _either_ a successful result _or_
an error. In Go, our function returned _both_, and we relied on the caller
knowing that the "result" would not contain any useful data if the function
returned an error.

Rust also requires intermediate functions to explicitly propagate errors, but it
adds the ? operator, which helps remove some of the boilerplate involved.

[next slide]

And lastly, error handling is not specific to imperative languages; errors can
occur when you're using a functional language too. Here is what our example
looks like in Haskell. It's very similar to Rust, though the either type is
actually called "Either". And Haskell uses "do" notation to accomplish the same
task as Rust's question mark operator. (This works because Haskell's Either type
is a "monad" — another esoteric functional programming term, which we do _not_
have the time to dive into today.)


Resources and cleanup
---------------------

hero image: https://flic.kr/p/eRjo3W

Next we're going to talk about how you clean up after yourself. This is
especially important for long-running programs like servers. If you need to
allocate or acquire resources as part of handling a request, but don't release
those resources when you're done, you'll quickly exhaust your available
capacity.

[next slide]

The original resource that programs have had to manage since the dawn of
computing is memory. To start, we're going to look at what's needed in C, where
there are no language facilities to help you. Here you can see a data type,
along with helper functions for allocating and deallocating instances of this
type that live on the heap. Note how one of the fields itself needs to be
managed, since we make a copy of the person's name.

[next slide]

And here we can how you might use these functions to handle instances of this
new type. [next] But because you're managing resources manually, you'd better
make sure you free everything!

Now, I'm not gonna lie, this is very cumbersome to have to manually manage your
resources like this! But it does have one genuine benefit, too — you can see
in your code precisely where everything is acquired or released. Especially for
low-level or real-time programs, it can be useful to know that every instruction
that the CPU executes is due to code that you wrote.

However, it's fair to say that most programmers have decided that that benefit
is not worth the extra work needed to manage resources manually — at least for
memory!

[next slide]

If you want to avoid that manual bookkeeping, a simple choice would be to use a
language that does this bookkeeping for you. Here we see the same example in Go,
[next] and in Python, both of which use garbage collectors to manage memory for
you.

[next slide]

Garbage collection is not your only option! C++ "smart pointers" are another
approach that is just as automatic. Here's the C++ version that manually manages
memory, like our original C version. [next] And here's the C++ that uses
unique_ptr and shared_ptr, which automatically release the underlying memory
when the pointer goes out of scope. [next] Rust has a similar facility, but its
smart pointers are called "Box" and "Arc" instead of "unique_ptr" and
"shared_ptr".

Other resources?

Closing a file after you're done with it
GC finalizers don't work
Need defer or with
RAII in C++ and Rust

Make sure it works in all of your return paths!


Concurrency
-----------

hero image: https://flic.kr/p/NpqGS

OS threads
green threads / goroutines
evented: promises, futures, async
Just do it single threaded! (Coordination always has an overhead)


Conclusion
----------

hero image: https://flic.kr/p/NHDXRb

We've come to the end of our journey. We still have a multitude of programming
languages (and always will!). But hopefully I've been able to add a bit of order
to that multitude for you today.

If you've encountered a new style of programming that used to be completely
alien to you, consider if there are any parts of it that you can bring back to
your native language and ecosystem. If there aren't, you can at least be
thankful that you now have a better, more mindful appreciation for the ways
things are done in your language. And if you do find yourself with some spare
time for personal learning and experimentation, consider implementing a toy
project in a language that's completely outside your comfort zone. It will be
worth the effort!

Thanks for your time!
